Log files: logs/visa_wall_2700952.out/err (Mode: custom)
==========================================
THE VISA WALL: LLM BIAS EVALUATION
==========================================
Job ID: 2700952
Job Name: visa_wall_bias
Node: uc2n901
Partition: dev_gpu_a100_il
GPU(s): 4
CPUs: 
Memory:  MB
Workspace: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI
Start Time: Sun Dec 28 08:40:02 PM CET 2025
==========================================

Loading modules...
Modules loaded âœ“

Setting up Python environment...
Activated venv: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv

Model Cache Configuration:
  Model cache workspace: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache
  HF_HOME: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache/huggingface_cache
  Models will be cached persistently across jobs

Clearing stale caches from previous jobs...
  Cache cleanup complete âœ“

Cache directories configured (job-local in $TMPDIR):
  TMPDIR: /scratch/slurm_tmpdir/job_2700952
  SLURM_JOB_ID: 2700952
  HF_HOME: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache/huggingface_cache (persistent)
  TRANSFORMERS_CACHE: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache/huggingface_cache/hub (persistent)
  TORCH_COMPILE_CACHE_DIR: /scratch/slurm_tmpdir/job_2700952/torch_compile_2700952 (job-local)
  VLLM_CACHE_ROOT: /scratch/slurm_tmpdir/job_2700952/vllm_2700952 (job-local)

Environment Information:
  Python version: Python 3.11.7
  Python path: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/bin/python
  vLLM version: 0.13.0
  CUDA version: V12.8.61

GPU Information:
NVIDIA A100 80GB PCIe, 81920 MiB, 81132 MiB, 46
NVIDIA A100 80GB PCIe, 81920 MiB, 81132 MiB, 41
NVIDIA A100 80GB PCIe, 81920 MiB, 81132 MiB, 41
NVIDIA A100 80GB PCIe, 81920 MiB, 81132 MiB, 42
==========================================

Pre-flight Checks:
  Python script: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/run_bias_evaluation.py
  Config file: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/config.yaml
  Output dir: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/results
  All packages OK âœ“
==========================================

Run Mode: custom
Runs per candidate: 10
==========================================
Running custom models: llama31-70b
Extra-large model detected (70B) - using tensor_parallel=4, gpu_memory=0.85

Executing command:
python /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/run_bias_evaluation.py             --config /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/config.yaml             --models llama31-70b             --runs 10             --output /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/results             --gpu-memory 0.85             --tensor-parallel 4             --max-model-len 4096
==========================================
âœ“ Logged in to HuggingFace successfully
============================================================
THE VISA WALL: LLM Bias Evaluation
============================================================
Config: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/config.yaml
Models: ['llama31-70b']
Candidates: ['anonymous', 'lukas', 'andrei', 'mehmet', 'minh', 'wei']
Runs per candidate: 10
Output directory: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/results
Mitigation tests: True
Max model length: 4096

============================================================
Loading model: llama31-70b (meta-llama/Llama-3.1-70B-Instruct)
============================================================
  Using max_model_len: 4096
WARNING 12-28 20:41:59 [attention.py:82] Using VLLM_ATTENTION_BACKEND environment variable is deprecated and will be removed in v0.14.0 or v1.0.0, whichever is soonest. Please use --attention-config.backend command line argument or AttentionConfig(backend=...) config field instead.
INFO 12-28 20:41:59 [utils.py:253] non-default args: {'trust_remote_code': True, 'max_model_len': 4096, 'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'model': 'meta-llama/Llama-3.1-70B-Instruct'}
INFO 12-28 20:42:21 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 12-28 20:42:21 [model.py:1661] Using max model len 4096
INFO 12-28 20:42:25 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(EngineCore_DP0 pid=435619)[0;0m INFO 12-28 20:42:26 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='meta-llama/Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=meta-llama/Llama-3.1-70B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751] WorkerProc failed to start.
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 722, in worker_main
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]     worker = WorkerProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 553, in __init__
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]     self.worker.init_device()
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 216, in init_device
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]     current_platform.set_device(self.device)
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 123, in set_device
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]     torch.cuda.set_device(device)
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 567, in set_device
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751]     torch._C._cuda_setDevice(device)
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751] torch.AcceleratorError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751] Search for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:27 [multiproc_executor.py:751] 
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 660, in wait_for_ready
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866]     raise e from None
[0;36m(EngineCore_DP0 pid=435619)[0;0m ERROR 12-28 20:42:28 [core.py:866] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.

==========================================
âœ— Job failed with exit code: 1
  End Time: Sun Dec 28 08:42:30 PM CET 2025

Check logs for details:
  /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/logs/visa_wall_2700952.err
==========================================

============================= JOB FEEDBACK =============================

NodeName=uc2n901
Job ID: 2700952
Cluster: uc3
User/Group: ma_anhnnguy/ma_ma
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 128
CPU Utilized: 00:00:54
CPU Efficiency: 0.27% of 05:34:56 core-walltime
Job Wall-clock time: 00:02:37
Memory Utilized: 946.29 MB
Memory Efficiency: 0.19% of 498.05 GB (498.05 GB/node)
