Log files: logs/visa_wall_2698468.out/err (Mode: custom)
==========================================
THE VISA WALL: LLM BIAS EVALUATION
==========================================
Job ID: 2698468
Job Name: visa_wall_bias
Node: uc2n901
Partition: dev_gpu_a100_il
GPU(s): 1
CPUs: 
Memory:  MB
Workspace: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI
Start Time: Sun Dec 28 02:35:04 PM CET 2025
==========================================

Loading modules...
Modules loaded âœ“

Setting up Python environment...
Activated venv: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv

Model Cache Configuration:
  Model cache workspace: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache
  HF_HOME: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache/huggingface_cache
  Models will be cached persistently across jobs

Clearing stale caches from previous jobs...
  Cache cleanup complete âœ“

Cache directories configured (job-local in $TMPDIR):
  TMPDIR: /scratch/slurm_tmpdir/job_2698468
  SLURM_JOB_ID: 2698468
  HF_HOME: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache/huggingface_cache (persistent)
  TRANSFORMERS_CACHE: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache/huggingface_cache/hub (persistent)
  TORCH_COMPILE_CACHE_DIR: /scratch/slurm_tmpdir/job_2698468/torch_compile_2698468 (job-local)
  VLLM_CACHE_ROOT: /scratch/slurm_tmpdir/job_2698468/vllm_2698468 (job-local)

Environment Information:
  Python version: Python 3.11.7
  Python path: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/bin/python
  vLLM version: 0.13.0
  CUDA version: V12.8.61

GPU Information:
NVIDIA A100 80GB PCIe, 81920 MiB, 81132 MiB, 41
==========================================

Pre-flight Checks:
  Python script: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/run_bias_evaluation.py
  Config file: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/config.yaml
  Output dir: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/results
  All packages OK âœ“
==========================================

Run Mode: custom
Runs per candidate: 30
==========================================
Running custom models: gemma2-9b
Gemma model detected - adding --enforce-eager for softcapping support

Executing command:
python /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/run_bias_evaluation.py             --config /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/config.yaml             --models gemma2-9b             --runs 30             --output /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/results             --gpu-memory 0.90             --tensor-parallel 1             --max-model-len 4096 --enforce-eager
==========================================
âœ“ Logged in to HuggingFace successfully
============================================================
THE VISA WALL: LLM Bias Evaluation
============================================================
Config: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/config.yaml
Models: ['gemma2-9b']
Candidates: ['anonymous', 'lukas', 'andrei', 'mehmet', 'minh', 'wei']
Runs per candidate: 30
Output directory: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/results
Mitigation tests: True
Max model length: 4096

============================================================
Loading model: gemma2-9b (google/gemma-2-9b-it)
============================================================
  Using max_model_len: 4096
  Using enforce_eager=True (for Gemma 2 softcapping support)
WARNING 12-28 14:37:05 [attention.py:82] Using VLLM_ATTENTION_BACKEND environment variable is deprecated and will be removed in v0.14.0 or v1.0.0, whichever is soonest. Please use --attention-config.backend command line argument or AttentionConfig(backend=...) config field instead.
INFO 12-28 14:37:05 [utils.py:253] non-default args: {'trust_remote_code': True, 'max_model_len': 4096, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'google/gemma-2-9b-it'}
INFO 12-28 14:37:28 [model.py:514] Resolved architecture: Gemma2ForCausalLM
INFO 12-28 14:37:28 [model.py:1661] Using max model len 4096
INFO 12-28 14:37:31 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-28 14:37:31 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
INFO 12-28 14:37:31 [vllm.py:722] Cudagraph is disabled under eager mode
[0;36m(EngineCore_DP0 pid=396242)[0;0m INFO 12-28 14:37:33 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=google/gemma-2-9b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 216, in init_device
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]     current_platform.set_device(self.device)
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 123, in set_device
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]     torch.cuda.set_device(device)
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 567, in set_device
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866]     torch._C._cuda_setDevice(device)
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866] torch.AcceleratorError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866] Search for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[0;36m(EngineCore_DP0 pid=396242)[0;0m ERROR 12-28 14:37:34 [core.py:866] 

==========================================
âœ— Job failed with exit code: 1
  End Time: Sun Dec 28 02:37:35 PM CET 2025

Check logs for details:
  /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/logs/visa_wall_2698468.err
==========================================

============================= JOB FEEDBACK =============================

NodeName=uc2n901
Job ID: 2698468
Cluster: uc3
User/Group: ma_anhnnguy/ma_ma
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 32
CPU Utilized: 00:00:54
CPU Efficiency: 1.05% of 01:25:20 core-walltime
Job Wall-clock time: 00:02:40
Memory Utilized: 1.02 GB
Memory Efficiency: 0.82% of 124.51 GB (124.51 GB/node)
