Log files: logs/visa_wall_2698568.out/err (Mode: custom)
==========================================
THE VISA WALL: LLM BIAS EVALUATION
==========================================
Job ID: 2698568
Job Name: visa_wall_bias
Node: uc3n082
Partition: dev_gpu_h100
GPU(s): 1
CPUs: 
Memory:  MB
Workspace: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI
Start Time: Sun Dec 28 03:50:03 PM CET 2025
==========================================

Loading modules...
Modules loaded âœ“

Setting up Python environment...
Activated venv: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv

Model Cache Configuration:
  Model cache workspace: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache
  HF_HOME: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache/huggingface_cache
  Models will be cached persistently across jobs

Clearing stale caches from previous jobs...
  Cache cleanup complete âœ“

Cache directories configured (job-local in $TMPDIR):
  TMPDIR: /scratch/slurm_tmpdir/job_2698568
  SLURM_JOB_ID: 2698568
  HF_HOME: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache/huggingface_cache (persistent)
  TRANSFORMERS_CACHE: /pfs/work9/workspace/scratch/ma_anhnnguy-model_cache/huggingface_cache/hub (persistent)
  TORCH_COMPILE_CACHE_DIR: /scratch/slurm_tmpdir/job_2698568/torch_compile_2698568 (job-local)
  VLLM_CACHE_ROOT: /scratch/slurm_tmpdir/job_2698568/vllm_2698568 (job-local)

Environment Information:
  Python version: Python 3.11.7
  Python path: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/bin/python
  vLLM version: 0.13.0
  CUDA version: V12.8.61

GPU Information:
NVIDIA H100, 95830 MiB, 95304 MiB, 45
==========================================

Pre-flight Checks:
  Python script: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/run_bias_evaluation.py
  Config file: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/config.yaml
  Output dir: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/results
  All packages OK âœ“
==========================================

Run Mode: custom
Runs per candidate: 30
==========================================
Running custom models: gemma2-9b
Gemma model detected - adding --enforce-eager for softcapping support

Executing command:
python /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/run_bias_evaluation.py             --config /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/config.yaml             --models gemma2-9b             --runs 30             --output /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/results             --gpu-memory 0.90             --tensor-parallel 1             --max-model-len 4096 --enforce-eager
==========================================
âœ“ Logged in to HuggingFace successfully
============================================================
THE VISA WALL: LLM Bias Evaluation
============================================================
Config: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/config.yaml
Models: ['gemma2-9b']
Candidates: ['anonymous', 'lukas', 'andrei', 'mehmet', 'minh', 'wei']
Runs per candidate: 30
Output directory: /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/results
Mitigation tests: True
Max model length: 4096

============================================================
Loading model: gemma2-9b (google/gemma-2-9b-it)
============================================================
  Using max_model_len: 4096
  Using enforce_eager=True (for Gemma 2 softcapping support)
WARNING 12-28 15:52:04 [attention.py:82] Using VLLM_ATTENTION_BACKEND environment variable is deprecated and will be removed in v0.14.0 or v1.0.0, whichever is soonest. Please use --attention-config.backend command line argument or AttentionConfig(backend=...) config field instead.
INFO 12-28 15:52:04 [utils.py:253] non-default args: {'trust_remote_code': True, 'max_model_len': 4096, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'google/gemma-2-9b-it'}
INFO 12-28 15:52:16 [model.py:514] Resolved architecture: Gemma2ForCausalLM
INFO 12-28 15:52:16 [model.py:1661] Using max model len 4096
INFO 12-28 15:52:19 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-28 15:52:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
INFO 12-28 15:52:19 [vllm.py:722] Cudagraph is disabled under eager mode
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:52:21 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=google/gemma-2-9b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:52:22 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.21.224.85:49239 backend=nccl
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:52:23 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:52:32 [gpu_model_runner.py:3562] Starting to load model google/gemma-2-9b-it...
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:52:33 [cuda.py:315] Using AttentionBackendEnum.FLASH_ATTN backend.
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:52:46 [default_loader.py:308] Loading weights took 11.81 seconds
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:52:47 [gpu_model_runner.py:3659] Model loading took 17.2181 GiB memory and 13.509661 seconds
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:53:01 [gpu_worker.py:375] Available KV cache memory: 57.00 GiB
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:53:01 [kv_cache_utils.py:1291] GPU KV cache size: 177,872 tokens
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:53:01 [kv_cache_utils.py:1296] Maximum concurrency for 4,096 tokens per request: 43.34x
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:53:17 [core.py:259] init engine (profile, create kv cache, warmup model) took 30.24 seconds
[0;36m(EngineCore_DP0 pid=233354)[0;0m WARNING 12-28 15:53:17 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=233354)[0;0m INFO 12-28 15:53:17 [vllm.py:722] Cudagraph is disabled under eager mode
INFO 12-28 15:53:17 [llm.py:360] Supported tasks: ['generate']

  Evaluating: anonymous (run 1/30)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [dump_input.py:72] Dumping input data for V1 LLM engine (v0.13.0) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=google/gemma-2-9b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}, 
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [dump_input.py:79] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=0,prompt_token_ids_len=225,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.9, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>', '<|im_end|>', '</s>'], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]),num_computed_tokens=0,lora_request=None,prompt_embeds_shape=None)], scheduled_cached_reqs=CachedRequestData(req_ids=[], resumed_req_ids=[], new_token_ids=[], all_token_ids={}, new_block_ids=[], num_computed_tokens=[], num_output_tokens=[]), num_scheduled_tokens={0: 225}, total_num_scheduled_tokens=225, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[0, 15], finished_req_ids=[], free_encoder_mm_hashes=[], preempted_req_ids=[], pending_structured_output_tokens=false, kv_connector_metadata=null, ec_connector_metadata=null)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868] EngineCore encountered a fatal error.
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 859, in run_engine_core
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     engine_core.run_busy_loop()
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 886, in run_busy_loop
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     self._process_engine_step()
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 919, in _process_engine_step
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     outputs, model_executed = self.step_fn()
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]                               ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 351, in step
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     model_output = future.result()
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]                    ^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/opt/bwhpc/common/devel/python/3.11.7_gnu_14.2/lib/python3.11/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return self.__get_result()
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/opt/bwhpc/common/devel/python/3.11.7_gnu_14.2/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     raise self._exception
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 79, in collective_rpc
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 369, in execute_model
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return self.worker.execute_model(scheduler_output, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 623, in execute_model
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     output = self.model_runner.execute_model(
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3087, in execute_model
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     model_output = self._model_forward(
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]                    ^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 2750, in _model_forward
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return self.model(
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py", line 418, in forward
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 372, in __call__
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return self.forward(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py", line 311, in forward
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     hidden_states, residual = layer(
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]                               ^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py", line 242, in forward
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     hidden_states = self.self_attn(
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]                     ^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py", line 184, in forward
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     attn_output = self.attn(q, k, v)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]                   ^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/attention/layer.py", line 347, in forward
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     torch.ops.vllm.unified_attention_with_output(
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/_ops.py", line 1255, in __call__
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py", line 39, in wrapper
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/attention/layer.py", line 863, in unified_attention_with_output
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     self.impl.forward(
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py", line 687, in forward
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     flash_attn_varlen_func(
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py", line 278, in flash_attn_varlen_func
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]   File "/pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/venv/lib/python3.11/site-packages/torch/_ops.py", line 1255, in __call__
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=233354)[0;0m ERROR 12-28 15:53:18 [core.py:868] RuntimeError: This flash attention build does not support tanh softcapping.

==========================================
âœ— Job failed with exit code: 1
  End Time: Sun Dec 28 03:53:20 PM CET 2025

Check logs for details:
  /pfs/work9/workspace/scratch/ma_anhnnguy-topic_modeling_data/RAI/logs/visa_wall_2698568.err
==========================================

============================= JOB FEEDBACK =============================

NodeName=uc3n082
Job ID: 2698568
Cluster: uc3
User/Group: ma_anhnnguy/ma_ma
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 48
CPU Utilized: 00:01:08
CPU Efficiency: 0.66% of 02:52:48 core-walltime
Job Wall-clock time: 00:03:36
Memory Utilized: 2.00 GB
Memory Efficiency: 1.06% of 188.77 GB (188.77 GB/node)
